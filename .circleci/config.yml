version: 2.1
# Use a package of configuration called an orb.
orbs:
  # Choose either one of the orbs below
  # welcome: circleci/welcome-orb@0.4.1
  aws-cli: circleci/aws-cli@3.1.3
# Define the jobs we want to run for this project


commands:
  # Exercise: Reusable Job Code
  # Exercise - Rollback
  destroy_aws_environment:
    steps:
      - run:
          # ${CIRCLE_WORKFLOW_ID} is a Built-in environment variable 
          # ${CIRCLE_WORKFLOW_ID:0:7} takes the first 5 chars of the variable CIRCLE_CI_WORKFLOW_ID 
          name: Destroy AWS Environment
          when: on_fail
          command: |
            aws cloudformation delete-stack --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:7}




jobs:
  # Exercise: Infrastructure Creation
  # Exercise - Rollback
  create_infrastructure: 
      docker:
        - image: amazon/aws-cli
      # executor: aws-cli/default
      steps:
        - checkout
        - run:
            name: Install tar utility
            command: |
              yum install -y tar gzip
        - aws-cli/setup:
            profile-name: default
        - run:
            name: Create Cloudformation Stack
            command: |
              aws cloudformation deploy \
                --template-file template.yml \
                --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:7} \
                --region us-east-1
        - destroy_aws_environment
        - run: touch ~/inventory.txt
        - run: echo [all] > inventory.txt
        - run: aws ec2 describe-instances --query 'Reservations[*].Instances[*].PublicIpAddress' --output text >> ~/inventory.txt
        - run: cat ~/inventory.txt
        - persist_to_workspace:
            root: ~/
            paths: 
              - inventory.txt


  # Exercise: Config and Deployment
  configure_infrastructure: 
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - add_ssh_keys: 
          fingerprints: ["ba:5c:84:48:e6:4e:22:59:a5:59:17:12:e1:ac:df:69"]
      - run:
          name: install dependencies
          command: | 
            # install the dependencies needed for your playbook
            apk add --update ansible
      - attach_workspace:
          at: ~/
      - run: cat ~/inventory.txt
      - run:
          name: Configure Server
          command: |
            ansible-playbook -i ~/inventory.txt playbook.yml
  
  # Exercise: Smoke Testing
  smoke_test_curl:
    docker:
      - image: alpine:latest
    steps:
      - run: apk add --update curl
      - run:
          name: smoke test
          command: |
            URL="https://blog.udacity.com/"
            # Test if website exists
            if curl -s --head ${URL} 
            then
              return 0
            else
              return 1
            fi
# Exercise: Smoke Testing
  smoke_test_forced_fail:
    docker:
      - image: cibuilds/aws:1.16.1
    steps:
      - run:
          name: Test job
          # Fail the job intentionally to simulate an error.
          command:  return 1
      - destroy_aws_environment


  # Executes the bucket.yml - Deploy an S3 bucket, and interface with that bucket to synchronize the files between local and the bucket.
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the bucket.yml template file.
  create_and_deploy_front_end_bucket:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute bucket.yml - Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
            --template-file bucket.yml \
            --stack-name stack-create-bucket-${CIRCLE_WORKFLOW_ID:0:7} \
            --parameter-overrides MyBucketName="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"
  # Uncomment the step below if yoou wish to upload all contents of the current directory to the S3 bucket
      - run: aws s3 sync ./front-end s3://mybucket-${CIRCLE_WORKFLOW_ID:0:7} --delete


  # Fetch and save the pipeline ID (bucket ID) responsible for the last release.
  get_last_deployment_id:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - run:
          name: Fetch and save the old pipeline ID (bucket name) responsible for the last release.
          command: |
            aws cloudformation \
            list-exports --query "Exports[?Name==\`PipelineID\`].Value" \
            --no-paginate --output text > ~/textfile.txt
      - persist_to_workspace:
          root: ~/
          paths: 
            - textfile.txt 

# Executes the cloudfront.yml template that will modify the existing CloudFront Distribution, change its target from the old bucket to the new bucket - `mybucket-${CIRCLE_WORKFLOW_ID:0:7}`. 
# Notice here we use the stack name `production-distro` which is the same name we used while deploying to the S3 bucket manually.
  promote_to_production:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute cloudfront.yml
          command: |
            aws cloudformation deploy \
            --template-file cloudfront.yml \
            --stack-name production-distro \
            --parameter-overrides PipelineID="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"

# Destroy the previous production version's S3 bucket and CloudFormation stack. 
  clean_up_old_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - attach_workspace:
          at: ~/
      - run:
          name: Destroy the previous S3 bucket and CloudFormation stack. 
          # Use $OldBucketID environment variable or mybucket644752792305 below.
          # Similarly, you can create and use $OldStackID environment variable in place of production-distro 
          command: |
            export OldBucketID=$(cat ~/textfile.txt)
            aws s3 rm "s3://${OldBucketID}" --recursive



workflows:

    # Name the workflow
#   myWorkflow1:
#     jobs:
#       - create_infrastructure
#       #- configure_infrastructure:
#       #    requires:
#       #      - create_infrastructure
#       #- smoke_test_forced_fail:
#       #    requires:
#       #      - create_infrastructure


   # Name the workflow
   myWorkflow2:
    jobs:
      - create_and_deploy_front_end_bucket
      - get_last_deployment_id
      - promote_to_production:
          requires:
            - create_and_deploy_front_end_bucket
      - clean_up_old_front_end:
          requires:
            - promote_to_production
            - get_last_deployment_id